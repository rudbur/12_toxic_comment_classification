{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076c17dc",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Цель\" data-toc-modified-id=\"Цель-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Цель</a></span></li><li><span><a href=\"#План-действий\" data-toc-modified-id=\"План-действий-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>План действий</a></span></li></ul></li><li><span><a href=\"#Spacy\" data-toc-modified-id=\"Spacy-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Spacy</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9dce9",
   "metadata": {},
   "source": [
    "# Классификация текстов методами ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00b43b",
   "metadata": {},
   "source": [
    "### Цель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31366dd2",
   "metadata": {},
   "source": [
    "На основе корпуса с более 150тыс. документов построить модель ML для предсказания их токсичности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d06915",
   "metadata": {},
   "source": [
    "### План действий"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca9f57",
   "metadata": {},
   "source": [
    "- очистка корпуса регулярными выражениями\n",
    "- распознавание корпуса при помощи Spacy и его доочистка\n",
    "- эмбеддинг Spacy и LGBM\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be5e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install unidecode -U\n",
    "#!pip3 install contractions -U\n",
    "#!pip3 install spacy -U\n",
    "#!pip3 install mapply -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01776ec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#преобразование в юникод\n",
    "import unidecode\n",
    "#работа с апострофами(don't -> do not)\n",
    "import contractions\n",
    "#многопоточный apply\n",
    "import mapply\n",
    "mapply.init(n_workers=-1)\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94dc4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('toxic_comments.csv')\n",
    "except FileNotFoundError:\n",
    "    df = pd.read_csv('datasets/toxic_comments.csv')\n",
    "display(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a62d364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic_0': 0.8983211235124177, 'toxic_1': 0.10167887648758234}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#соотношение категорий в корпусе\n",
    "_=df.toxic.value_counts(normalize=True).values\n",
    "{'toxic_0':_[0],'toxic_1':_[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8a447e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e396613f779946cd8f6247ea34e38e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''очистка регулярными выражениями\n",
    "оставляем знаки препинания для Spacy, чтобы лучше понимала предложения.\n",
    "'''\n",
    "\n",
    "corpus = df.text\n",
    "def clear_text(text):\n",
    "    #преобразование к юникоду и нижнему регистру\n",
    "    _ = unidecode.unidecode(text.lower()) \n",
    "    \n",
    "    #замена спец.символов, веб-ссылок, хэштегов, на точки\n",
    "    _ = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+|\\n|\\r|\\t', '. ', _)\n",
    "    \n",
    "    #удаление всего, кроме основных символов\n",
    "    _ = re.sub(r'[^a-z,.!\\? \\']',' ', _) \n",
    "    \n",
    "    #работа с апострофами\n",
    "    _ = contractions.fix(_) \n",
    "    _ = re.sub(r'\\'',' ', _)\n",
    "    \n",
    "    #удаление пробелов перед точками\n",
    "    _ = re.sub(r'\\s+\\.', '.', _)\n",
    "    \n",
    "    #ограничение на повторение символа (2 подряд)\n",
    "    _ = re.sub(r'(.)\\1{2,}', '\\\\1\\\\1', _ )\n",
    "    \n",
    "    #удаление лишних точек\n",
    "    _ = re.sub(r'\\.{2,}', '.', _)\n",
    "    #_ = re.sub(r'\\b\\w\\b', '', _)\n",
    "    \n",
    "    #удаление слов с длиной больше 20\n",
    "    _ = re.sub(r'\\b\\w{20,}\\b', '', _)\n",
    "    _ = ' '.join([word for word in _.split()])\n",
    "    return _\n",
    "corpus = corpus.mapply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352c9d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation. why the edits made under my username hardcore metallica fan were reverted? they were not vandalisms, just closure on some gas after i voted at new york dolls fac. and please do not remove the template from the talk page since i am retired now.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c360305c",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889d798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''загрузка словаря spacy и пайплайн для работы с документами (исключим из него распознавание названий)\n",
    "более 500тыс. векторов в словаре, длина каждого 300\n",
    "'''\n",
    "\n",
    "#!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load(('en_core_web_lg'), disable=['ner', 'custom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e16cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [00:04, 127.40it/s]/home/rudbur/anaconda3/envs/praktikum/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/rudbur/anaconda3/envs/praktikum/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "159571it [12:48, 207.58it/s]\n"
     ]
    }
   ],
   "source": [
    "'''проходимся генератором по корпусу, преобразуя каждый документ в чистый текст и в вектор\n",
    "(среднее векторов слов). Если документ оказывается без вектора - заменяем его нулевым вектором длиной 300\n",
    "'''\n",
    "spacy_gen = nlp.pipe(corpus, n_process=-1, batch_size=100)\n",
    "corpus_texts = []\n",
    "corpus_vectors = []\n",
    "for doc in tqdm(spacy_gen):\n",
    "    doc_vector = np.mean([_.vector for _ in doc if (_.has_vector and not _.is_stop and not _.is_punct)], axis=0)\n",
    "    if not doc_vector.shape:\n",
    "        doc_vector = np.array([0]*300)\n",
    "    doc_text = ' '.join(\n",
    "        [_.lemma_ for _ in doc if (_.has_vector and not _.is_stop and not _.is_punct and len(_.lemma_)>2)])\n",
    "    corpus_texts.append(doc_text)\n",
    "    corpus_vectors.append(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88361d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#делим корпус на обучающую и тестовую выборки, следя за соотношениями в них целевого признака\n",
    "corpus_vectors = pd.DataFrame(corpus_vectors)\n",
    "corpus_texts = pd.Series(corpus_texts)\n",
    "X_train, X_test, y_train, y_test, X_texts_train, X_texts_test = train_test_split(\n",
    "    corpus_vectors, df.toxic, corpus_texts, test_size=.25, stratify=df.toxic, random_state=42)\n",
    "\n",
    "del corpus_vectors, corpus_texts, df, nlp, spacy_gen, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee9d024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CV_f1_score: 0.738376896109436'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test_f1_score: 0.7409274193548387'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 29s, sys: 3.18 s, total: 4min 32s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators':[500],\n",
    "    'learning_rate':[.1],\n",
    "    'num_leaves':[60]\n",
    "}\n",
    "\n",
    "spacy_lgbm = LGBMClassifier(\n",
    "    objective='binary', class_weight='balanced',\n",
    "    boosting_type='goss', random_state=42, force_col_wise=True\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(spacy_lgbm, scoring='f1', n_jobs=-1, refit=True, verbose=2, random_state=42, n_iter=1,\n",
    "                           cv=3, param_distributions=param_distributions)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "display(f'CV_f1_score: {search.best_score_}')\n",
    "display(f'test_f1_score: {f1_score(y_test, search.best_estimator_.predict(X_test))}')\n",
    "\n",
    "del spacy_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975da969",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae39aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пайплайн из TFIDF и линейной регрессии с подбором гиперпараметров (лучшие захардкодены)\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "logreg = LogisticRegression(class_weight='balanced', n_jobs=-1, max_iter=500)\n",
    "pipe = Pipeline([('tfidf', tfidf), ('logreg', logreg)])\n",
    "param_distributions = {\n",
    "    'tfidf__ngram_range':[(1,2)],\n",
    "    'tfidf__max_features':[200000],\n",
    "    'tfidf__max_df':[.4],\n",
    "    'logreg__C':[6]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipe, scoring='f1', n_jobs=-1, refit=True, verbose=2, random_state=42, n_iter=25,\n",
    "                           cv=3, param_distributions=param_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1684c0ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rudbur/anaconda3/envs/praktikum/lib/python3.7/site-packages/sklearn/model_selection/_search.py:296: UserWarning: The total space of parameters 1 is smaller than n_iter=25. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7763801537386443"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_texts_train,y_train)\n",
    "f1_score(y_test, search.best_estimator_.predict(X_texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaacebf",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde3b9a",
   "metadata": {},
   "source": [
    "Наиболее быстрой и точной оказалась модель TF-IDF + Регрессия, `f1 = 0.77`\n",
    "\n",
    "- корпус был очищен от символов не из юникода, ссылок, хэштегов, повторяющихся символов методом регулярных выражения\n",
    "- далее корпус был подан на вход Spacy. Распознав его, оставили в корпусе только леммы слов, для которых в словаре Spacy есть вектор, и которые не являются стоп-словами, знаками препинания и длиной больше 2 символов\n",
    "- Spacy-вектора документов (300 признаков) отправили в LGBM, получив `f1 = 0.74`\n",
    "- затем применили другой метод, проанализировав очищенный корпус методом TF-IDF и подобрав оптимальные гиперпараметры\n",
    "- Логистическая регресссия дала `f1 = 0.77`"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 19591,
    "start_time": "2022-07-31T05:28:40.746Z"
   },
   {
    "duration": 81727,
    "start_time": "2022-07-31T05:29:00.339Z"
   },
   {
    "duration": 2521,
    "start_time": "2022-07-31T05:30:22.068Z"
   },
   {
    "duration": 8,
    "start_time": "2022-07-31T05:30:24.592Z"
   },
   {
    "duration": 29326,
    "start_time": "2022-07-31T05:30:24.602Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-31T05:30:53.930Z"
   },
   {
    "duration": 81744,
    "start_time": "2022-07-31T05:30:53.936Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
